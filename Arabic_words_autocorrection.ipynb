{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUI6lpzVYR6A"
      },
      "source": [
        "## Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUt6sgNTzRLA",
        "outputId": "6fe2d4e6-3d4d-43e9-a3b7-ce9f276f1fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnupGRtSYR6J"
      },
      "source": [
        "## Cleaning Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZmTm-tbzsG3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    # Remove leading digits and any whitespace\n",
        "    cleaned_sentence = re.sub(r'^\\d+\\s*', '', sentence)\n",
        "    # Remove any leading or trailing whitespace\n",
        "    cleaned_sentence = cleaned_sentence.strip()\n",
        "    return cleaned_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vccx0cXYR6M"
      },
      "source": [
        "## Uploading Correct Sentences(sentences) and wrong Sentences(labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VczFPzZGzsNc"
      },
      "outputs": [],
      "source": [
        "# Read the file and store sentences in a list\n",
        "y = []\n",
        "with open(\"/content/drive/My Drive/ara-eg_newscrawl-OSIAN_2018_10K-sentences.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        cleaned_line = clean_sentence(line)\n",
        "        if cleaned_line:  # Check if the line is not empty after cleaning\n",
        "            y.append(cleaned_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yktBZm9DzsPi"
      },
      "outputs": [],
      "source": [
        "# Read the file and store sentences in a list\n",
        "x = []\n",
        "with open(\"/content/drive/My Drive/ara-eg_newscrawl-OSIAN_2018_10K-labels.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        cleaned_line = clean_sentence(line)\n",
        "        if cleaned_line:  # Check if the line is not empty after cleaning\n",
        "            x.append(cleaned_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZrH-FcGzsRv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    # Remove English words using regular expression\n",
        "    cleaned_sentence = re.sub(r'\\b[a-zA-Z]+\\b', '', sentence)\n",
        "    # Remove numbers and signs using regular expression\n",
        "    cleaned_sentence = re.sub(r'\\d+|\\W+', ' ', cleaned_sentence)\n",
        "    # Remove punctuation marks\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]', '', cleaned_sentence)\n",
        "    # Normalize diacritics (fatha, kasra, damma)\n",
        "    cleaned_sentence = cleaned_sentence.replace('ً', '').replace('ٌ', '').replace('ٍ', '').replace('َ', '').replace('ُ', '').replace('ِ', '')\n",
        "    # Remove special characters\n",
        "    cleaned_sentence= re.sub(r'[^\\u0600-\\u06FF\\s\\d]', '', cleaned_sentence)\n",
        "    # Remove numbers\n",
        "    cleaned_sentence= re.sub(r'\\d+', '', cleaned_sentence)\n",
        "    # Normalize whitespace\n",
        "    cleaned_sentence= re.sub(r'\\s+', ' ', cleaned_sentence.strip())\n",
        "    # Remove extra whitespace\n",
        "    cleaned_sentence = ' '.join(cleaned_sentence.split())\n",
        "    return cleaned_sentence\n",
        "\n",
        "# Clean each sentence in the list\n",
        "x = [clean_sentence(sentence) for sentence in x]\n",
        "# Clean each sentence in the list\n",
        "y = [clean_sentence(sentence) for sentence in y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vhdvNpAYR6R"
      },
      "source": [
        "## Get the 1st 5 sentence from both correct and wrong sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bq514OzsTm",
        "outputId": "b7011d36-3053-4f17-cbe6-6a383e61ea40",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['مليارات و مليون درهم خلال تكل الفترة', 'معلومات خاطئة نعتقد للأسنان جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنان', 'مسلسل يونس ولد بطولة عمرو سعد وسيعرض على قناة مصر دراما المستقبل السومرية', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حثي ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان ببه']\n",
            "['مليارات و مليون درهم خلال تلك الفترة', 'معلومات خاطئة نعتقد أنها جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنان', 'مسلسل يونس ولد فضة بطولة عمرو سعد وسيعرض على قناة مصر دراما المستقبل السومرية', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حيث ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان ببه']\n"
          ]
        }
      ],
      "source": [
        "print(x[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwjlUIYcYR6S"
      },
      "source": [
        "## Import important libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9Nb31gLYR6U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCSfCPZGYR6U"
      },
      "source": [
        "## Getting seek into the model, Injoy.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420,
          "referenced_widgets": [
            "e707687ded134adfa2d0b40d47808c2b",
            "342d5f42f4b942fc9e846c917794332f",
            "9ae5ca7f6abb4fe186c16969e1689b63",
            "3751f8f69dbf4bf6ab10dcd388e1a5f8",
            "eaa8f709b07546ddaefca686cf702dd3",
            "a004f6facf5b40d6a4ae6d3606b6e293",
            "1ab39caf00664baf885ed8ae60ad3200",
            "58d479e19f6c47538d73f8f4d2f39bfb",
            "b462c873b9f14f9097cc13a430d80649",
            "df61814dd93b40f5a9fedbd5eaa2e8fe",
            "53346481bf534783ad0a7b29ccba5eeb",
            "bc25219b5bf14998b996561fd65a0b2e",
            "6a328acdbaf54032918c8e6b1f6ab970",
            "ea74414cf77a447a8e19d3369b5fa2a2",
            "5ce05162a34741c6aa27c2e31c826edb",
            "19f792c2f7c94a0982ce5b648c061bc7",
            "54a92115863e43d2b6bbfbbc8c509a2e",
            "04a95c47e631427ab4e5b14db3621c1b",
            "da350ef0f7c9439980403c3d21012e0f",
            "ca07e9d516bb40d984f804e57430128a",
            "db0600bbab374e65b5a14351db72337c",
            "1478f6750c19445ab51e9e3ef7560f6d",
            "827fdbdad0a646a4a259c7db8108df2d",
            "1b233ef831b8461cada10d6b472f89d3",
            "45369e032dd24887a4d7625c50c8eeed",
            "07ff8880caec414db66f9e467733c9e7",
            "b2944bbc952448f2a989caf6299047ac",
            "2a63db193c76408eb31de733aa9b213f",
            "71a909ea2863451fb5023745cf9067a8",
            "10dcea1ce8cf48bdb75f8145d637da6d",
            "3e97b296cc934ced83b8633a2ad9a9ac",
            "a0e83d7cb3b44b9db4019457bfa9f7e6",
            "9825231b73c04b888b109fbdcc9638a7",
            "6e2736de8def4cc0b39c64b523a07964",
            "3bfeaf4170154488aac8e45e5ed5cace",
            "ec40e42fa24d493cb5cc9c25d7256796",
            "b4028b09533b46bfb5b0a15d719a3e1d",
            "dbae510b26b542338c2a7ed57acb7a7a",
            "8065280f4c264102bd2f0ecefc7f70e2",
            "607fbd3bedd2415b9d80a4b1ce268cdd",
            "6138ff76b70c4975af9b060ef5c0e0bb",
            "d0edb4ed0553444da36d3297df8a5057",
            "80fe5f738c104d53b92f69230e024cac",
            "a01b21709d0443549bd5b3824fd77ad7",
            "fc4492823ad1402fb3a13ff1bd31d48f",
            "aac5d5239abc47b7a5596aa9e5121ed2",
            "e93dcb2ec11f4a5292f71685f36cdd27",
            "bbc922d3dbf84424ae6038d326fed852",
            "5408a6753b7747858f2d5ac7fe1533f5",
            "2e98c820ae584fd6b99ed0c9c3914c0d",
            "c1604403a97e42e99718ce154fe410a5",
            "62ec665c89c443a2908e18feeddca49f",
            "8b3d9a2f535749c38c9967227bc204a3",
            "599489060022429480c9fa5caa78a467",
            "60e6b0a3c975449faedc958d6411d810",
            "ad368f84bfb44902855bde0958c9ace7",
            "13dc5a502ac84bb7a3b58f48613025b1",
            "dab8191b930747c0b7e689b7f9269f0d",
            "cc399a4a51c54a8b83568c4ded3cc6d9",
            "470c745a3e82463681c7bb8dabc9a3cf",
            "1482befd2c9942f8ac5bdc3deab166d2",
            "50c23659d50144069c85e0d656743cc3",
            "18aaec4fe8b44928b9c571a34589b488",
            "ff0df1e6290740ac80b1979da5897116",
            "b3ca9b43a02843a4936aa03a367a586e",
            "b2d582276ea744f19fdb99e9bc7793ef"
          ]
        },
        "id": "CO5Rq5i0YR6a",
        "outputId": "69298b24-543d-4b65-9983-c282a48dcc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e707687ded134adfa2d0b40d47808c2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc25219b5bf14998b996561fd65a0b2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/717k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "827fdbdad0a646a4a259c7db8108df2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.26M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e2736de8def4cc0b39c64b523a07964"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4492823ad1402fb3a13ff1bd31d48f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad368f84bfb44902855bde0958c9ace7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabert were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained AraBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabert\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"aubmindlab/bert-base-arabert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7e3LtO0YR6b"
      },
      "outputs": [],
      "source": [
        "# Add special tokens to the sentences\n",
        "max_length = 32  # Define maximum sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub4lfpkQYR6c"
      },
      "outputs": [],
      "source": [
        "# Tokenize the sentences (assuming 'x' and 'y' are your input and target sequences)\n",
        "tokenized_x = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "tokenized_y = tokenizer(y, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRbQuYp0YR6j"
      },
      "outputs": [],
      "source": [
        "# Convert tokenized sentences to PyTorch tensors\n",
        "input_ids = tokenized_x.input_ids\n",
        "labels = tokenized_y.input_ids\n",
        "\n",
        "# Create a PyTorch Dataset\n",
        "dataset = TensorDataset(input_ids, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouSBReLVYR6m"
      },
      "outputs": [],
      "source": [
        "# Define training parameters\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 5\n",
        "batch_size = 8\n",
        "num_folds = 5  # Number of folds for cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgr2yDqXYR6y"
      },
      "outputs": [],
      "source": [
        "# Initialize KFold\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHLC_b-xYR6y"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store metrics for each fold\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS1Ebjn5YR6z",
        "outputId": "54572e0e-ba00-4936-90ed-9412f6d819d9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1, Epoch 1, Train Loss: 1.5191736900806427, Train Accuracy: 0.73163671875\n",
            "Fold 1, Epoch 2, Train Loss: 1.0432270367890597, Train Accuracy: 0.7752890625\n",
            "Fold 1, Epoch 3, Train Loss: 0.7492960204929113, Train Accuracy: 0.82969921875\n",
            "Fold 1, Epoch 4, Train Loss: 0.5369095611236989, Train Accuracy: 0.87153125\n",
            "Fold 1, Epoch 5, Train Loss: 0.40296816068701447, Train Accuracy: 0.89820703125\n",
            "Fold 1, Test Loss: 1.4670731830000878, Test Accuracy: 0.7973125\n",
            "Fold 2/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2, Epoch 1, Train Loss: 0.577788025084883, Train Accuracy: 0.87509765625\n",
            "Fold 2, Epoch 2, Train Loss: 0.34849737108498813, Train Accuracy: 0.9137578125\n",
            "Fold 2, Epoch 3, Train Loss: 0.26100907327234746, Train Accuracy: 0.93108203125\n",
            "Fold 2, Epoch 4, Train Loss: 0.20169293646980077, Train Accuracy: 0.9448203125\n",
            "Fold 2, Epoch 5, Train Loss: 0.1624084329821635, Train Accuracy: 0.95409375\n",
            "Fold 2, Test Loss: 0.4265996302496642, Test Accuracy: 0.9123125\n",
            "Fold 3/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3, Epoch 1, Train Loss: 0.2294270624020137, Train Accuracy: 0.9419765625\n",
            "Fold 3, Epoch 2, Train Loss: 0.15946081203175708, Train Accuracy: 0.9567265625\n",
            "Fold 3, Epoch 3, Train Loss: 0.12032410067424644, Train Accuracy: 0.9662578125\n",
            "Fold 3, Epoch 4, Train Loss: 0.09967291083699092, Train Accuracy: 0.9723984375\n",
            "Fold 3, Epoch 5, Train Loss: 0.08295027286699042, Train Accuracy: 0.97697265625\n",
            "Fold 3, Test Loss: 0.1523933961223811, Test Accuracy: 0.96015625\n",
            "Fold 4/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4, Epoch 1, Train Loss: 0.1242829478806816, Train Accuracy: 0.96719921875\n",
            "Fold 4, Epoch 2, Train Loss: 0.0850342912771739, Train Accuracy: 0.9768046875\n",
            "Fold 4, Epoch 3, Train Loss: 0.06704074032942299, Train Accuracy: 0.9820546875\n",
            "Fold 4, Epoch 4, Train Loss: 0.0697156179837184, Train Accuracy: 0.9814921875\n",
            "Fold 4, Epoch 5, Train Loss: 0.06043793439131696, Train Accuracy: 0.98402734375\n",
            "Fold 4, Test Loss: 0.07152686445764266, Test Accuracy: 0.979953125\n",
            "Fold 5/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5, Epoch 1, Train Loss: 0.07268637857493013, Train Accuracy: 0.98083984375\n",
            "Fold 5, Epoch 2, Train Loss: 0.04792855208704714, Train Accuracy: 0.987265625\n",
            "Fold 5, Epoch 3, Train Loss: 0.05662169781263219, Train Accuracy: 0.9853671875\n",
            "Fold 5, Epoch 4, Train Loss: 0.06116278507676907, Train Accuracy: 0.98465625\n",
            "Fold 5, Epoch 5, Train Loss: 0.05472746467031538, Train Accuracy: 0.98653515625\n",
            "Fold 5, Test Loss: 0.03394576233671978, Test Accuracy: 0.992125\n",
            "Average Test Loss: 0.4303077672332991, Average Test Accuracy: 0.9283718749999998\n"
          ]
        }
      ],
      "source": [
        "# Cross-validation loop\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold {fold+1}/{num_folds}\")\n",
        "\n",
        "    # Split dataset into train and test sets for this fold\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
        "    test_dataset = torch.utils.data.Subset(dataset, test_index)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Set device to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for this fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, labels = batch\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate predictions and true labels\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Convert the train_labels and train_preds lists to NumPy arrays\n",
        "        train_labels = np.array(train_labels)\n",
        "        train_preds = np.array(train_preds)\n",
        "\n",
        "        # Calculate average training loss for the epoch\n",
        "        train_avg_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "        # Calculate the accuracy score\n",
        "        train_accuracy = accuracy_score(train_labels.ravel(), train_preds.ravel())\n",
        "        print(f\"Fold {fold+1}, Epoch {epoch+1}, Train Loss: {train_avg_loss}, Train Accuracy: {train_accuracy}\")\n",
        "\n",
        "    # Test for this fold\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, labels = batch\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate predictions and true labels\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert the test_labels and test_preds lists to NumPy arrays\n",
        "    test_labels = np.array(test_labels)\n",
        "    test_preds = np.array(test_preds)\n",
        "\n",
        "    # Calculate average test loss for the fold\n",
        "    test_avg_loss = test_loss / len(test_dataloader)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    test_accuracy = accuracy_score(test_labels.ravel(), test_preds.ravel())\n",
        "    print(f\"Fold {fold+1}, Test Loss: {test_avg_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Append metrics for this fold to the lists\n",
        "    test_losses.append(test_avg_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Print average test loss and accuracy across all folds\n",
        "print(f\"Average Test Loss: {np.mean(test_losses)}, Average Test Accuracy: {np.mean(test_accuracies)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ungAoo5-YR60"
      },
      "source": [
        "## Try the model after cross validation by testing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJrF9U0yYR63",
        "outputId": "23fe604d-a9d8-43a3-ad49-271c9b4a8343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: صودا الخبز هو معجون طبيعي للأسنان حثي ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان ببه\n",
            "Corrected sentence: صودا الخبز هو معجون طبيعي للأسنان حيث ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وحمر الأسنان ببه ب\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "def auto_correct_sentence(model, tokenizer, sentence, device):\n",
        "    # Tokenize the input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Move inputs to device\n",
        "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
        "\n",
        "    # Forward pass through the model\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Get probabilities using softmax\n",
        "    probabilities = softmax(logits, dim=2)\n",
        "\n",
        "    # Get the predicted labels\n",
        "    _, predicted_labels = torch.max(probabilities, dim=2)\n",
        "\n",
        "    # Decode predicted labels to tokens\n",
        "    corrected_tokens = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in predicted_labels]\n",
        "\n",
        "    # Join tokens to form corrected sentence\n",
        "    corrected_sentence = \" \".join(corrected_tokens)\n",
        "\n",
        "    return corrected_sentence\n",
        "\n",
        "# Example usage\n",
        "def auto_correct_sentences(model, tokenizer, sentences, device):\n",
        "    corrected_sentences = []\n",
        "    for sentence in sentences:\n",
        "        corrected_sentence = auto_correct_sentence(model, tokenizer, sentence, device)\n",
        "        corrected_sentences.append(corrected_sentence)\n",
        "    return corrected_sentences\n",
        "\n",
        "# Example usage\n",
        "sentences = [\"صودا الخبز هو معجون طبيعي للأسنان حثي ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان ببه\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device\n",
        "corrected_sentences = auto_correct_sentences(model, tokenizer, sentences, device)\n",
        "print(\"Original sentence:\", sentences[0])\n",
        "print(\"Corrected sentence:\", corrected_sentences[0])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
